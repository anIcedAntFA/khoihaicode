
# Upload Large File Excel - Best Practices & Implementation Strategies

> **X·ª≠ l√Ω upload file Excel 1 tri·ªáu records v·ªõi c√°c pattern backend ph√π h·ª£p** üìä

## üéØ **B√†i to√°n th·ª±c t·∫ø**

**Scenario**: User upload file Excel 1 tri·ªáu records ‚Üí Insert v√†o database ‚Üí Cung c·∫•p real-time progress cho user

**Challenges**:
- File size l·ªõn (c√≥ th·ªÉ 50-100MB+)
- Processing time l√¢u (5-30 ph√∫t)
- Memory consumption cao
- User experience (kh√¥ng bi·∫øt ƒëang ·ªü ƒë√¢u)
- Error handling (n·∫øu fail ·ªü record 500k th√¨ sao?)

---

## üèóÔ∏è **System Design Diagrams**

### **Overall System Architecture**

```mermaid
graph TB
    subgraph "Client Layer"
        UI[Web Browser/Mobile App]
        JS[JavaScript Client]
    end
    
    subgraph "API Gateway Layer"
        LB[Load Balancer]
        API1[API Server 1]
        API2[API Server 2]
        API3[API Server 3]
    end
    
    subgraph "Processing Layer"
        Q[Job Queue<br/>Bull/Redis]
        W1[Worker 1]
        W2[Worker 2]
        W3[Worker 3]
        FS[File Storage<br/>Disk/S3]
    end
    
    subgraph "Data Layer"
        DB[(Database<br/>MongoDB/PostgreSQL)]
        CACHE[(Redis Cache<br/>Job Status)]
        LOGS[(Logs<br/>Progress Events)]
    end
    
    subgraph "Real-time Layer"
        SSE[SSE Connections]
        WS[WebSocket Pool]
    end
    
    %% Connections
    UI --> LB
    LB --> API1
    LB --> API2
    LB --> API3
    
    API1 --> Q
    API2 --> Q
    API3 --> Q
    
    Q --> W1
    Q --> W2
    Q --> W3
    
    W1 --> DB
    W2 --> DB
    W3 --> DB
    
    W1 --> FS
    W2 --> FS
    W3 --> FS
    
    API1 --> SSE
    API2 --> SSE
    API3 --> SSE
    
    W1 --> CACHE
    W2 --> CACHE
    W3 --> CACHE
    
    SSE --> UI
    
    %% Styling
    classDef client fill:#e1f5fe
    classDef api fill:#f3e5f5
    classDef process fill:#e8f5e8
    classDef data fill:#fff3e0
    classDef realtime fill:#fce4ec
    
    class UI,JS client
    class LB,API1,API2,API3 api
    class Q,W1,W2,W3,FS process
    class DB,CACHE,LOGS data
    class SSE,WS realtime
```

### **Component Responsibilities**

| Component | Responsibility | Technology |
|-----------|---------------|------------|
| **Load Balancer** | Route requests, health checks | Nginx/HAProxy |
| **API Servers** | Handle HTTP requests, validate files | Express.js/Node.js |
| **Job Queue** | Store and distribute tasks | Bull.js + Redis |
| **Workers** | Process Excel files, batch insert | Node.js processes |
| **File Storage** | Temporary file storage | Local disk/AWS S3 |
| **Database** | Store final records | MongoDB/PostgreSQL |
| **Cache** | Job status, progress tracking | Redis |
| **SSE Layer** | Real-time progress updates | Server-Sent Events |

---

## üìä **Detailed Sequence Flows**

### **Flow 1: File Upload & Job Creation**

```mermaid
sequenceDiagram
    participant U as User Browser
    participant API as API Server
    participant V as File Validator
    participant Q as Job Queue
    participant DB as Database
    participant FS as File Storage
    
    Note over U: User selects Excel file
    U->>API: POST /api/upload/excel<br/>[multipart file data]
    
    activate API
    Note over API: Receive file upload
    API->>FS: Save temporary file
    FS-->>API: File path
    
    API->>V: Validate file
    activate V
    V->>V: Check file format
    V->>V: Quick row count
    V->>V: Validate headers
    V-->>API: Validation result
    deactivate V
    
    alt File Invalid
        API-->>U: 400 Bad Request<br/>{"error": "Invalid file"}
    else File Valid
        API->>API: Generate jobId
        Note over API: Create job metadata
        
        API->>DB: Store job record
        activate DB
        DB-->>API: Job saved
        deactivate DB
        
        API->>Q: Add processing job
        activate Q
        Note over Q: Job queued for processing
        Q-->>API: Job queued
        deactivate Q
        
        API-->>U: 200 Success<br/>{"jobId": "123", "totalRows": 50000}
    end
    deactivate API
    
    Note over U: Immediately start progress tracking
```

### **Flow 2: Background Processing with Progress Updates**

```mermaid
sequenceDiagram
    participant Q as Job Queue
    participant W as Worker Process
    participant FS as File Storage
    participant DB as Database
    participant CACHE as Redis Cache
    participant SSE as SSE Broadcast
    participant U as User Browser
    
    Note over W: Worker picks up job
    Q->>W: Dequeue job
    activate W
    
    W->>CACHE: Update status: "processing"
    W->>FS: Read Excel file
    FS-->>W: File data stream
    
    W->>W: Parse Excel headers
    W->>W: Split into batches (1000 rows)
    
    Note over W: Processing loop starts
    loop For each batch (1000 rows)
        W->>W: Validate batch records
        W->>W: Transform data format
        
        W->>DB: Batch insert records
        activate DB
        alt Insert Success
            DB-->>W: Success: {insertedIds: [...]}
            Note over W: Update progress counters
            W->>W: successRows += 1000
        else Insert Partial Failure
            DB-->>W: Partial: {inserted: 850, failed: 150}
            Note over W: Handle failed records
            W->>W: successRows += 850
            W->>W: failedRows += 150
        end
        deactivate DB
        
        W->>CACHE: Update job progress
        Note over CACHE: Store current stats
        
        W->>SSE: Broadcast progress event
        activate SSE
        SSE->>U: SSE Event: batch_complete
        Note over U: Update progress bar<br/>Show recent IDs
        deactivate SSE
        
        Note over W: Small delay (100ms)
        W->>W: await sleep(100)
    end
    
    Note over W: All batches processed
    W->>CACHE: Update status: "completed"
    W->>FS: Delete temporary file
    
    W->>SSE: Broadcast completion
    activate SSE
    SSE->>U: SSE Event: completed
    Note over U: Show final summary<br/>Close progress UI
    deactivate SSE
    
    deactivate W
```

### **Flow 3: Real-time Progress Tracking (SSE)**

```mermaid
sequenceDiagram
    participant U as User Browser
    participant API as API Server
    participant CACHE as Redis Cache
    participant SSE as SSE Manager
    participant W as Worker Process
    
    Note over U: After upload success
    U->>API: GET /api/progress/{jobId}/stream
    activate API
    
    API->>CACHE: Get current job status
    CACHE-->>API: Job progress data
    
    API->>SSE: Register connection
    activate SSE
    Note over SSE: Store connection mapping
    SSE-->>API: Connection established
    
    API-->>U: SSE Headers<br/>Content-Type: text/event-stream
    Note over U: EventSource connected
    
    API->>U: SSE: connection event
    API->>U: SSE: current progress
    deactivate API
    
    Note over W: Worker processing in background
    
    loop Every batch completion
        W->>CACHE: Update progress
        W->>SSE: Trigger broadcast
        
        SSE->>SSE: Find connections for jobId
        SSE->>U: SSE: batch_complete event<br/>data: {percentage, insertedIds}
        
        Note over U: JavaScript updates UI
        U->>U: updateProgressBar(percentage)
        U->>U: showRecentInserts(insertedIds)
    end
    
    alt Processing completed
        W->>SSE: Final completion event
        SSE->>U: SSE: completed event
        Note over U: Show success summary
        U->>API: Close SSE connection
        API->>SSE: Remove connection
        deactivate SSE
    else Processing failed
        W->>SSE: Error event
        SSE->>U: SSE: error event
        Note over U: Show error message
    end
```

### **Flow 4: Error Handling & Recovery**

```mermaid
sequenceDiagram
    participant W as Worker Process
    participant DB as Database
    participant Q as Job Queue
    participant CACHE as Redis Cache
    participant SSE as SSE Broadcast
    participant U as User Browser
    participant LOG as Error Logger
    
    Note over W: Processing batch 500/1000
    W->>DB: Insert batch records
    
    alt Database Connection Error
        DB-->>W: Connection timeout
        W->>LOG: Log error details
        W->>CACHE: Update: attempt 1/3
        W->>Q: Requeue job (retry delay: 2s)
        W->>SSE: Broadcast retry event
        SSE->>U: SSE: retrying event
        Note over U: Show "Retrying..." message
        
    else Validation Errors (Partial)
        DB-->>W: 850 success, 150 failed
        W->>LOG: Log validation failures
        W->>W: Continue with next batch
        W->>SSE: Broadcast partial success
        SSE->>U: SSE: batch_partial event
        Note over U: Show warnings
        
    else Critical System Error
        DB-->>W: Fatal error (disk full)
        W->>LOG: Log critical error
        W->>CACHE: Update status: "failed"
        W->>Q: Mark job as failed
        W->>SSE: Broadcast failure
        SSE->>U: SSE: failed event
        Note over U: Show error + retry option
        
    else Max Retries Reached
        W->>CACHE: Update status: "failed"
        W->>LOG: Log max retries exceeded
        W->>SSE: Final failure event
        SSE->>U: SSE: failed event
        Note over U: Show error summary<br/>Suggest manual retry
    end
```

### **Flow 5: Frontend State Management**

```mermaid
stateDiagram-v2
    [*] --> FileSelection: User opens page
    
    FileSelection --> Uploading: User selects file
    Uploading --> UploadSuccess: File uploaded
    Uploading --> UploadError: Upload failed
    
    UploadSuccess --> Connecting: Connect to SSE
    Connecting --> Processing: SSE connected
    Connecting --> ConnectionError: SSE failed
    
    Processing --> Processing: Progress updates
    Processing --> Completed: All batches done
    Processing --> ProcessingError: Processing failed
    Processing --> Retrying: Temporary failure
    
    Retrying --> Processing: Retry successful
    Retrying --> Failed: Max retries exceeded
    
    ProcessingError --> Retrying: Auto retry
    ProcessingError --> Failed: Fatal error
    
    ConnectionError --> Connecting: Retry connection
    ConnectionError --> Polling: Fallback to polling
    
    Polling --> Completed: Check status success
    Polling --> Failed: Check status failed
    
    Completed --> [*]: Show summary
    Failed --> [*]: Show error
    UploadError --> [*]: Show upload error
    
    note right of Processing
        States with real-time updates:
        - batch_complete
        - progress_update
        - error_partial
    end note
    
    note right of Retrying
        Auto transitions:
        - attempt 1/3 (2s delay)
        - attempt 2/3 (4s delay)  
        - attempt 3/3 (8s delay)
    end note
```

---

## ‚öôÔ∏è **Data Flow Architecture**

### **Data Processing Pipeline**

```mermaid
flowchart TD
    A[Excel File Upload] --> B{File Validation}
    B -->|Valid| C[Create Job Record]
    B -->|Invalid| D[Return Error]
    
    C --> E[Add to Queue]
    E --> F[Worker Picks Job]
    
    F --> G[Read Excel Stream]
    G --> H[Parse Headers]
    H --> I[Split into Batches]
    
    I --> J[Process Batch]
    J --> K{Validate Records}
    
    K -->|Valid Records| L[Batch Insert DB]
    K -->|Invalid Records| M[Collect Errors]
    
    L --> N{Insert Result}
    N -->|Success| O[Update Counters]
    N -->|Partial Success| P[Handle Failures]
    
    O --> Q[Broadcast Progress]
    P --> Q
    M --> Q
    
    Q --> R{More Batches?}
    R -->|Yes| J
    R -->|No| S[Final Summary]
    
    S --> T[Cleanup Files]
    T --> U[Broadcast Completion]
    
    subgraph "Error Handling"
        V[Connection Error] --> W[Retry Logic]
        W --> X{Max Retries?}
        X -->|No| F
        X -->|Yes| Y[Mark Failed]
    end
    
    subgraph "Real-time Updates"
        Q --> Z[SSE Broadcast]
        Z --> AA[Update UI]
        AA --> AB[Progress Bar]
        AA --> AC[Statistics]
        AA --> AD[Recent IDs]
    end
```

### **Memory & Performance Optimization**

```mermaid
graph LR
    subgraph "Memory Management"
        A[Excel File<br/>100MB] --> B[Stream Reader<br/>1000 rows/chunk]
        B --> C[Memory Buffer<br/>~5MB max]
        C --> D[Process & Clear<br/>Garbage Collection]
    end
    
    subgraph "Database Optimization"
        E[Batch Insert<br/>1000 records] --> F[Connection Pool<br/>5-10 connections]
        F --> G[Transaction<br/>Commit per batch]
        G --> H[Index Optimization<br/>Background]
    end
    
    subgraph "Scaling Strategy"
        I[Multiple Workers] --> J[Queue Distribution]
        J --> K[Load Balancing]
        K --> L[Horizontal Scaling]
    end
    
    D --> E
    H --> I
```

---

## üìã **Recommended Solution: Hybrid Pattern**

### **Step 1: File Upload (Request-Response)**
```typescript
// POST /api/upload/excel
interface UploadResponse {
    success: boolean;
    jobId: string;
    fileName: string;
    totalRows: number; // Estimated from file
    message: string;
}
```

### **Step 2: Background Processing (Job Queue)**
```typescript
interface ProcessingJob {
    jobId: string;
    filePath: string;
    userId: string;
    totalRows: number;
    status: 'pending' | 'processing' | 'completed' | 'failed';
    processedRows: number;
    successRows: number;
    failedRows: number;
    errors: string[];
}
```

### **Step 3: Progress Tracking (Server-Sent Events)**
```typescript
// GET /api/progress/{jobId}/stream
interface ProgressEvent {
    jobId: string;
    type: 'progress' | 'batch_complete' | 'error' | 'completed';
    data: {
        totalRows: number;
        processedRows: number;
        successRows: number;
        failedRows: number;
        percentage: number;
        currentBatch: number;
        estimatedTimeRemaining?: string;
        errors?: string[];
        insertedIds?: string[]; // IDs of last 1000 inserted records
    };
}
```

---

## ü§î **T·∫°i sao SSE kh√¥ng ph√π h·ª£p cho File Upload?**

### **Hi·ªÉu l·∫ßm th∆∞·ªùng g·∫∑p:**
> "SSE kh√¥ng ph√π h·ª£p cho file uploads" **KH√îNG c√≥ nghƒ©a** l√† kh√¥ng d√πng SSE trong lu·ªìng upload

### **√ù nghƒ©a th·ª±c t·∫ø:**

1. **‚ùå SSE kh√¥ng th·ªÉ upload file:**
   ```javascript
   // WRONG - Kh√¥ng th·ªÉ l√†m nh∆∞ n√†y
   eventSource.send(fileData); // SSE ch·ªâ receive, kh√¥ng send
   ```

2. **‚úÖ SSE d√πng cho progress tracking:**
   ```javascript
   // CORRECT - D√πng SSE ƒë·ªÉ track progress
   // Upload qua HTTP POST ‚Üí Track progress qua SSE
   ```

### **L√Ω do k·ªπ thu·∫≠t:**

| Aspect | HTTP Upload | SSE |
|--------|------------|-----|
| **Direction** | Client ‚Üí Server | Server ‚Üí Client |
| **Data Size** | Large files (GB) | Small messages (KB) |
| **Purpose** | Transfer data | Stream updates |
| **Protocol** | POST multipart | GET text/event-stream |
| **Buffering** | Chunked upload | Text messages |

---

## üîê **User Authentication & Targeting trong SSE**

### **Challenge: L√†m sao bi·∫øt event g·ª≠i cho user n√†o?**

### **Solution 1: Query Parameter Authentication**
```typescript
// Client k·∫øt n·ªëi v·ªõi token
const eventSource = new EventSource(`/api/progress/${jobId}/stream?token=${userToken}`);

// Server verify
app.get('/api/progress/:jobId/stream', (req, res) => {
    const token = req.query.token;
    const user = verifyToken(token);
    
    if (!user) {
        return res.status(401).json({ error: 'Unauthorized' });
    }
    
    // Store connection v·ªõi userId
    connections.set(user.id, res);
});
```

### **Solution 2: Cookie-based Authentication**
```typescript
// Client t·ª± ƒë·ªông g·ª≠i cookie
const eventSource = new EventSource('/api/progress/stream');

// Server ƒë·ªçc cookie
app.get('/api/progress/stream', authenticateUser, (req, res) => {
    const userId = req.user.id; // From cookie middleware
    
    // Store connection
    userConnections.set(userId, res);
});
```

### **Solution 3: Connection Mapping Strategy**
```typescript
// Advanced: Map connections theo multiple criteria
const connectionMap = new Map<string, {
    userId: string;
    jobId: string;
    response: express.Response;
    connectedAt: Date;
}>();

function broadcastToUser(userId: string, data: any) {
    connectionMap.forEach((conn, connId) => {
        if (conn.userId === userId) {
            conn.response.write(`data: ${JSON.stringify(data)}\n\n`);
        }
    });
}

function broadcastToJob(jobId: string, data: any) {
    connectionMap.forEach((conn, connId) => {
        if (conn.jobId === jobId) {
            conn.response.write(`data: ${JSON.stringify(data)}\n\n`);
        }
    });
}
```

---

## ü§î **Deep Dive: Advanced Questions & Answers**

### **Q1: Job Queue l√† g√¨ v√† t·∫°i sao c·∫ßn n√≥?**

#### **Job Queue Overview:**
Job Queue l√† m·ªôt **message broker pattern** ƒë·ªÉ x·ª≠ l√Ω tasks n·∫∑ng trong background.

```mermaid
graph TD
    A[Client Request] --> B[API Server]
    B --> C[Add Job to Queue]
    C --> D[Return Immediately]
    
    E[Worker Process] --> F[Pick Job from Queue]
    F --> G[Process Job]
    G --> H[Update Status]
    
    I[Queue System] --> J[Redis/Database]
    J --> K[Job Storage]
```

#### **T·∫°i sao c·∫ßn Job Queue:**

| **Without Job Queue** | **With Job Queue** |
|----------------------|-------------------|
| üêå Client wait 30 ph√∫t | ‚ö° Client get response ngay l·∫≠p t·ª©c |
| üí• Server timeout | ‚úÖ Background processing |
| üîÑ Single-threaded | üöÄ Multi-worker scalable |
| ‚ùå No retry on failure | üîÑ Auto retry v·ªõi backoff |
| üìä No progress tracking | üìà Real-time progress |

#### **Implementation v·ªõi Bull Queue:**

```typescript
// 1. Setup Job Queue
import Bull from 'bull';
import Redis from 'ioredis';

const redis = new Redis({
    host: 'localhost',
    port: 6379,
    retryDelayOnFailover: 100
});

const excelQueue = new Bull('excel processing', {
    redis: {
        port: 6379,
        host: 'localhost'
    },
    defaultJobOptions: {
        removeOnComplete: 10,    // Keep last 10 completed jobs
        removeOnFail: 50,        // Keep last 50 failed jobs
        attempts: 3,             // Retry 3 times
        backoff: {
            type: 'exponential',
            delay: 2000,         // Start with 2s delay
        }
    }
});

// 2. Add Job to Queue
async function createExcelProcessingJob(jobData: ProcessingJob) {
    const job = await excelQueue.add('process-excel', jobData, {
        priority: jobData.totalRows > 100000 ? 1 : 10, // High priority for large files
        delay: 0,                                       // Process immediately
        jobId: jobData.jobId                           // Custom job ID
    });
    
    console.log(`Job ${job.id} added to queue`);
    return job;
}

// 3. Process Jobs
excelQueue.process('process-excel', 5, async (job) => {
    // 5 = concurrent workers
    const jobData: ProcessingJob = job.data;
    
    try {
        // Update status to processing
        await updateJobStatus(jobData.jobId, 'processing');
        
        // Your processing logic here
        await processExcelFile(jobData);
        
        // Mark as completed
        await updateJobStatus(jobData.jobId, 'completed');
        
    } catch (error) {
        await updateJobStatus(jobData.jobId, 'failed');
        throw error; // Let Bull handle retry
    }
});

// 4. Job Status Tracking
interface JobStatus {
    jobId: string;
    status: 'pending' | 'processing' | 'completed' | 'failed' | 'retrying';
    progress: number;
    attempts: number;
    error?: string;
    createdAt: Date;
    updatedAt: Date;
}
```

#### **Job Status Lifecycle:**

```mermaid
stateDiagram-v2
    [*] --> pending: Job Created
    pending --> processing: Worker picks up
    processing --> completed: Success
    processing --> failed: Error (max attempts)
    processing --> retrying: Error (will retry)
    retrying --> processing: Retry attempt
    retrying --> failed: Max attempts reached
    completed --> [*]
    failed --> [*]
```

**Status Meanings:**
- **`pending`**: Job ƒë√£ t·∫°o, ƒëang ch·ªù worker x·ª≠ l√Ω
- **`processing`**: Worker ƒë√£ pick up job v√† ƒëang x·ª≠ l√Ω
- **`retrying`**: Job failed nh∆∞ng s·∫Ω retry l·∫°i
- **`completed`**: X·ª≠ l√Ω th√†nh c√¥ng ho√†n to√†n
- **`failed`**: Fail v√† kh√¥ng retry n·ªØa

---

### **Q2: T·∫°i sao c√≥ failedRows trong ProgressEvent?**

#### **Partial Failure Strategy - Best Practice:**

Th·ª±c t·∫ø trong production, **kh√¥ng n√™n fail to√†n b·ªô job** ch·ªâ v√¨ 1 v√†i records l·ªói.

```typescript
// ‚ùå BAD: All-or-nothing approach
async function processBatch(records: any[]) {
    try {
        await db.insertMany(records); // If 1 record fails ‚Üí all 1000 fail
        return { success: 1000, failed: 0 };
    } catch (error) {
        return { success: 0, failed: 1000 }; // Lose all data!
    }
}

// ‚úÖ GOOD: Partial failure handling
async function processBatchWithPartialFailure(records: any[]) {
    const results = {
        successCount: 0,
        failedCount: 0,
        successIds: [] as string[],
        failedRecords: [] as { record: any, error: string }[]
    };
    
    // Strategy 1: Individual inserts with transaction
    const session = await db.startSession();
    session.startTransaction();
    
    try {
        for (const record of records) {
            try {
                // Validate record first
                const validatedRecord = await validateRecord(record);
                
                // Insert individual record
                const result = await db.collection('records').insertOne(validatedRecord, { session });
                
                results.successCount++;
                results.successIds.push(result.insertedId.toString());
                
            } catch (recordError) {
                results.failedCount++;
                results.failedRecords.push({
                    record,
                    error: recordError.message
                });
                
                // Log but continue processing
                console.warn(`Record failed:`, recordError.message);
            }
        }
        
        await session.commitTransaction();
        
    } finally {
        await session.endSession();
    }
    
    return results;
}

// Strategy 2: Batch insert with error collection
async function processBatchSmartly(records: any[]) {
    const validRecords = [];
    const invalidRecords = [];
    
    // Pre-validate all records
    for (const record of records) {
        try {
            const validated = await validateRecord(record);
            validRecords.push(validated);
        } catch (error) {
            invalidRecords.push({ record, error: error.message });
        }
    }
    
    // Insert valid records in batch
    let insertResult = null;
    if (validRecords.length > 0) {
        try {
            insertResult = await db.collection('records').insertMany(validRecords, {
                ordered: false // Continue on error
            });
        } catch (bulkError) {
            // Handle partial bulk insert failures
            console.error('Bulk insert error:', bulkError);
        }
    }
    
    return {
        successCount: insertResult?.insertedCount || 0,
        failedCount: invalidRecords.length,
        successIds: insertResult ? Object.values(insertResult.insertedIds) : [],
        failedRecords: invalidRecords
    };
}
```

#### **Real-world Example:**

```typescript
// Excel processing with partial failures
async function processExcelWithErrorHandling(jobData: ProcessingJob) {
    const BATCH_SIZE = 1000;
    let totalSuccess = 0;
    let totalFailed = 0;
    const allErrors: string[] = [];
    
    for (let i = 0; i < jobData.totalRows; i += BATCH_SIZE) {
        const batch = excelData.slice(i, i + BATCH_SIZE);
        
        const batchResult = await processBatchWithPartialFailure(batch);
        
        totalSuccess += batchResult.successCount;
        totalFailed += batchResult.failedCount;
        
        // Collect error details
        batchResult.failedRecords.forEach(failed => {
            allErrors.push(`Row ${i + records.indexOf(failed.record)}: ${failed.error}`);
        });
        
        // Broadcast progress (including failures)
        broadcastProgress(jobData.jobId, {
            type: 'batch_complete',
            data: {
                totalRows: jobData.totalRows,
                processedRows: i + batch.length,
                successRows: totalSuccess,
                failedRows: totalFailed,
                percentage: Math.round(((i + batch.length) / jobData.totalRows) * 100),
                currentBatch: Math.floor(i / BATCH_SIZE) + 1,
                insertedIds: batchResult.successIds,
                errors: batchResult.failedRecords.map(f => f.error).slice(0, 5) // Show recent errors
            }
        });
        
        // Continue processing even if some records failed
    }
    
    // Final summary
    const finalStatus = totalFailed > 0 ? 'completed_with_errors' : 'completed';
    
    broadcastProgress(jobData.jobId, {
        type: 'completed',
        data: {
            totalRows: jobData.totalRows,
            processedRows: jobData.totalRows,
            successRows: totalSuccess,
            failedRows: totalFailed,
            percentage: 100,
            errors: allErrors.slice(0, 10), // Show first 10 errors
            summary: `Processing completed: ${totalSuccess} success, ${totalFailed} failed`
        }
    });
}
```

#### **Benefits c·ªßa Partial Failure:**
- ‚úÖ **Data Recovery**: Kh√¥ng m·∫•t h·∫øt data v√¨ 1 v√†i records l·ªói
- ‚úÖ **User Experience**: User bi·∫øt ƒë∆∞·ª£c exact s·ªë l∆∞·ª£ng success/failed
- ‚úÖ **Debugging**: C√≥ th·ªÉ fix l·ªói v√† re-upload ch·ªâ failed records
- ‚úÖ **Business Value**: Partial data v·∫´n c√≥ gi√° tr·ªã s·ª≠ d·ª•ng

---

### **Q3: Protect JobId Security cho User**

#### **Security Concerns:**
```typescript
// ‚ùå VULNERABLE: Anyone can access any job
GET /api/progress/job_123456/stream  // No validation

// ‚úÖ SECURE: Only job owner can access
GET /api/progress/job_123456/stream  // With user validation
```

#### **Implementation: Multi-layer Security**

```typescript
// 1. Job Ownership Database Schema
interface JobRecord {
    jobId: string;
    userId: string;           // Owner of the job
    fileName: string;
    status: JobStatus;
    createdAt: Date;
    permissions: {
        canView: string[];    // UserIDs who can view
        canControl: string[]; // UserIDs who can pause/cancel
    };
    isPublic: boolean;        // For shared jobs
}

// 2. Authentication Middleware
async function authenticateUser(req: Request, res: Response, next: NextFunction) {
    try {
        const token = req.headers.authorization?.replace('Bearer ', '') || 
                     req.query.token || 
                     req.cookies.authToken;
        
        if (!token) {
            return res.status(401).json({ error: 'No authentication token provided' });
        }
        
        const decoded = jwt.verify(token, process.env.JWT_SECRET!);
        const user = await User.findById(decoded.userId);
        
        if (!user) {
            return res.status(401).json({ error: 'Invalid token' });
        }
        
        req.user = user;
        next();
    } catch (error) {
        return res.status(401).json({ error: 'Authentication failed' });
    }
}

// 3. Job Authorization Middleware
async function authorizeJobAccess(req: Request, res: Response, next: NextFunction) {
    try {
        const { jobId } = req.params;
        const userId = req.user.id;
        
        // Get job from database
        const job = await JobRecord.findOne({ jobId });
        
        if (!job) {
            return res.status(404).json({ error: 'Job not found' });
        }
        
        // Check ownership
        if (job.userId !== userId) {
            // Check if user has permission
            if (!job.permissions.canView.includes(userId) && !job.isPublic) {
                return res.status(403).json({ 
                    error: 'Access denied. You do not own this job.' 
                });
            }
        }
        
        // Add job to request for use in handler
        req.job = job;
        next();
        
    } catch (error) {
        return res.status(500).json({ error: 'Authorization check failed' });
    }
}

// 4. Secure SSE Endpoint
app.get('/api/progress/:jobId/stream', 
    authenticateUser,           // Step 1: Verify user identity
    authorizeJobAccess,         // Step 2: Check job access rights
    async (req, res) => {
        const { jobId } = req.params;
        const userId = req.user.id;
        const job = req.job;
        
        // Setup SSE
        res.writeHead(200, {
            'Content-Type': 'text/event-stream',
            'Cache-Control': 'no-cache',
            'Connection': 'keep-alive',
            'Access-Control-Allow-Origin': '*'
        });
        
        // Store connection with user context
        const connectionId = `${userId}_${jobId}_${Date.now()}`;
        progressConnections.set(connectionId, {
            response: res,
            userId,
            jobId,
            connectedAt: new Date(),
            permissions: job.permissions
        });
        
        // Send current status
        const currentProgress = await getJobProgress(jobId);
        if (currentProgress) {
            res.write(`data: ${JSON.stringify({
                type: 'progress',
                data: currentProgress
            })}\n\n`);
        }
        
        // Log access (for security auditing)
        console.log(`User ${userId} connected to job ${jobId} progress stream`);
        
        // Cleanup on disconnect
        req.on('close', () => {
            progressConnections.delete(connectionId);
            console.log(`User ${userId} disconnected from job ${jobId}`);
        });
    }
);

// 5. Secure Broadcast Function
function broadcastProgress(jobId: string, progressData: ProgressEvent) {
    progressConnections.forEach((connection, connectionId) => {
        if (connection.jobId === jobId) {
            try {
                if (!connection.response.writableEnded) {
                    // Additional data filtering based on permissions
                    const filteredData = filterDataByPermissions(progressData, connection.permissions);
                    
                    connection.response.write(`data: ${JSON.stringify(filteredData)}\n\n`);
                }
            } catch (error) {
                progressConnections.delete(connectionId);
            }
        }
    });
}

// 6. Permission-based Data Filtering
function filterDataByPermissions(data: ProgressEvent, permissions: any): ProgressEvent {
    // Example: Hide detailed error messages from view-only users
    if (!permissions.canControl.includes(data.userId)) {
        if (data.data.errors) {
            data.data.errors = data.data.errors.map(() => 'Error occurred (details hidden)');
        }
    }
    
    return data;
}

// 7. Additional Security: Rate Limiting
import rateLimit from 'express-rate-limit';

const progressStreamLimit = rateLimit({
    windowMs: 15 * 60 * 1000, // 15 minutes
    max: 10, // Max 10 SSE connections per user per 15 minutes
    keyGenerator: (req) => req.user.id,
    message: 'Too many progress stream connections. Please try again later.'
});

app.get('/api/progress/:jobId/stream', 
    progressStreamLimit,        // Rate limiting
    authenticateUser,
    authorizeJobAccess,
    // ... SSE handler
);

// 8. Job Sharing Feature (Optional)
interface ShareJobRequest {
    jobId: string;
    shareWithUsers: string[];   // UserIDs
    permissions: 'view' | 'control';
    expiresAt?: Date;
}

app.post('/api/jobs/:jobId/share', 
    authenticateUser,
    authorizeJobAccess,
    async (req, res) => {
        const { jobId } = req.params;
        const { shareWithUsers, permissions, expiresAt }: ShareJobRequest = req.body;
        
        // Update job permissions
        await JobRecord.updateOne(
            { jobId, userId: req.user.id }, // Only owner can share
            {
                $addToSet: {
                    [`permissions.${permissions === 'control' ? 'canControl' : 'canView'}`]: {
                        $each: shareWithUsers
                    }
                },
                sharedAt: new Date(),
                sharedBy: req.user.id,
                expiresAt
            }
        );
        
        res.json({ success: true, message: 'Job shared successfully' });
    }
);
```

#### **Security Best Practices:**

```typescript
// 1. Environment-based Security
const SECURITY_CONFIG = {
    JWT_SECRET: process.env.JWT_SECRET,
    JWT_EXPIRES_IN: '24h',
    MAX_CONCURRENT_JOBS_PER_USER: 5,
    MAX_SSE_CONNECTIONS_PER_USER: 3,
    JOB_RETENTION_DAYS: 30
};

// 2. Audit Logging
interface SecurityAuditLog {
    userId: string;
    action: 'job_created' | 'job_accessed' | 'job_shared' | 'unauthorized_access';
    jobId: string;
    ip: string;
    userAgent: string;
    timestamp: Date;
    details?: any;
}

function logSecurityEvent(event: SecurityAuditLog) {
    // Log to security monitoring system
    console.log('SECURITY_AUDIT:', JSON.stringify(event));
    
    // Store in database for analysis
    SecurityLog.create(event);
}

// 3. Input Validation
import Joi from 'joi';

const jobIdSchema = Joi.string().uuid().required();

function validateJobId(req: Request, res: Response, next: NextFunction) {
    const { error } = jobIdSchema.validate(req.params.jobId);
    if (error) {
        return res.status(400).json({ error: 'Invalid job ID format' });
    }
    next();
}
```

**Security Layers Summary:**
1. **Authentication**: JWT tokens verify user identity
2. **Authorization**: Database checks job ownership
3. **Rate Limiting**: Prevent abuse of SSE connections
4. **Input Validation**: Validate jobId format
5. **Audit Logging**: Track all access attempts
6. **Permission System**: Fine-grained access control
7. **Data Filtering**: Hide sensitive info based on permissions

---

## üéØ **Conclusion**

**Recommended Architecture cho Excel Upload 1M records:**

1. **HTTP POST** cho file upload (fast, reliable)
2. **Job Queue** cho background processing (scalable, fault-tolerant)
3. **SSE** cho real-time progress updates (user-friendly)
4. **Database batching** cho performance (efficient)

**Key Points:**
- SSE ho√†n to√†n ph√π h·ª£p cho **progress tracking**, kh√¥ng ph√π h·ª£p cho **file transfer**
- Authentication trong SSE th√¥ng qua query params ho·∫∑c cookies
- Job-based processing cho reliability v√† scalability
- Batch processing cho database performance

C√°ch ti·∫øp c·∫≠n n√†y balance gi·ªØa performance, user experience, v√† system reliability! üéØ
